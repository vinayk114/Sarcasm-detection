{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db0b2079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a577a7",
   "metadata": {},
   "source": [
    "### step 1: Dataset Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a9e21f",
   "metadata": {},
   "source": [
    "Explore the student comment & parent comment features by creating a text corpus. Which all cleaning operation you think will be required on this corpus? Write a clean-up method and clean the text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31a58715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "data = pd.read_csv('sarcasm_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c2de0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                0\n",
       "comment           0\n",
       "date              0\n",
       "down              0\n",
       "parent_comment    0\n",
       "score             0\n",
       "top               0\n",
       "topic             0\n",
       "user              0\n",
       "label             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7c8c5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'comment', 'date', 'down', 'parent_comment', 'score', 'top',\n",
       "       'topic', 'user', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53b076cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop insignificant columns\n",
    "data_sarc=data.drop(['ID','date', 'down', 'score', 'top',\n",
    "       'topic', 'user'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46927af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well, let's be honest here, they don't actuall...</td>\n",
       "      <td>They should shut the fuck up and let the commu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well, I didn't need evidence to believe in com...</td>\n",
       "      <td>You need evidence to kill people? I thought we...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who does an \"official promo\" in 360p?</td>\n",
       "      <td>2014 BMW S1000R: Official Promo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Grotto koth was the best</td>\n",
       "      <td>Not really that memorable lol if you want memo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neal's back baby</td>\n",
       "      <td>James Neal hit on Zach Parise</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0  Well, let's be honest here, they don't actuall...   \n",
       "1  Well, I didn't need evidence to believe in com...   \n",
       "2              Who does an \"official promo\" in 360p?   \n",
       "3                           Grotto koth was the best   \n",
       "4                                   Neal's back baby   \n",
       "\n",
       "                                      parent_comment  label  \n",
       "0  They should shut the fuck up and let the commu...      0  \n",
       "1  You need evidence to kill people? I thought we...      1  \n",
       "2                    2014 BMW S1000R: Official Promo      0  \n",
       "3  Not really that memorable lol if you want memo...      1  \n",
       "4                      James Neal hit on Zach Parise      1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the filtered data\n",
    "data_sarc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e9f0621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment', 'parent_comment', 'label'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sarc.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5990cb5a",
   "metadata": {},
   "source": [
    "### create text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64d37d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a text corpus\n",
    "data_sarc[\"text_corpus\"] = data_sarc[\"comment\"] + \" \" + data_sarc[\"parent_comment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6ca388c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>label</th>\n",
       "      <th>text_corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well, let's be honest here, they don't actuall...</td>\n",
       "      <td>They should shut the fuck up and let the commu...</td>\n",
       "      <td>0</td>\n",
       "      <td>Well, let's be honest here, they don't actuall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well, I didn't need evidence to believe in com...</td>\n",
       "      <td>You need evidence to kill people? I thought we...</td>\n",
       "      <td>1</td>\n",
       "      <td>Well, I didn't need evidence to believe in com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who does an \"official promo\" in 360p?</td>\n",
       "      <td>2014 BMW S1000R: Official Promo</td>\n",
       "      <td>0</td>\n",
       "      <td>Who does an \"official promo\" in 360p? 2014 BMW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Grotto koth was the best</td>\n",
       "      <td>Not really that memorable lol if you want memo...</td>\n",
       "      <td>1</td>\n",
       "      <td>Grotto koth was the best Not really that memor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neal's back baby</td>\n",
       "      <td>James Neal hit on Zach Parise</td>\n",
       "      <td>1</td>\n",
       "      <td>Neal's back baby James Neal hit on Zach Parise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0  Well, let's be honest here, they don't actuall...   \n",
       "1  Well, I didn't need evidence to believe in com...   \n",
       "2              Who does an \"official promo\" in 360p?   \n",
       "3                           Grotto koth was the best   \n",
       "4                                   Neal's back baby   \n",
       "\n",
       "                                      parent_comment  label  \\\n",
       "0  They should shut the fuck up and let the commu...      0   \n",
       "1  You need evidence to kill people? I thought we...      1   \n",
       "2                    2014 BMW S1000R: Official Promo      0   \n",
       "3  Not really that memorable lol if you want memo...      1   \n",
       "4                      James Neal hit on Zach Parise      1   \n",
       "\n",
       "                                         text_corpus  \n",
       "0  Well, let's be honest here, they don't actuall...  \n",
       "1  Well, I didn't need evidence to believe in com...  \n",
       "2  Who does an \"official promo\" in 360p? 2014 BMW...  \n",
       "3  Grotto koth was the best Not really that memor...  \n",
       "4     Neal's back baby James Neal hit on Zach Parise  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the dataset having text corpus\n",
    "data_sarc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ce22c8",
   "metadata": {},
   "source": [
    "### cleaning of text corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188b6886",
   "metadata": {},
   "source": [
    " #### cleaning operations required are\n",
    "    1. convert to lower characters\n",
    "    \n",
    "    2. remove stopwords\n",
    "    \n",
    "    3.remove panctuation\n",
    "    \n",
    "    4.remove special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b49ca2a",
   "metadata": {},
   "source": [
    "#### convert it into lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecf1bf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text corpusinto lower text\n",
    "data_sarc['text_corpus']=data_sarc['text_corpus'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bda31eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>label</th>\n",
       "      <th>text_corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well, let's be honest here, they don't actuall...</td>\n",
       "      <td>They should shut the fuck up and let the commu...</td>\n",
       "      <td>0</td>\n",
       "      <td>well, let's be honest here, they don't actuall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well, I didn't need evidence to believe in com...</td>\n",
       "      <td>You need evidence to kill people? I thought we...</td>\n",
       "      <td>1</td>\n",
       "      <td>well, i didn't need evidence to believe in com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who does an \"official promo\" in 360p?</td>\n",
       "      <td>2014 BMW S1000R: Official Promo</td>\n",
       "      <td>0</td>\n",
       "      <td>who does an \"official promo\" in 360p? 2014 bmw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Grotto koth was the best</td>\n",
       "      <td>Not really that memorable lol if you want memo...</td>\n",
       "      <td>1</td>\n",
       "      <td>grotto koth was the best not really that memor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neal's back baby</td>\n",
       "      <td>James Neal hit on Zach Parise</td>\n",
       "      <td>1</td>\n",
       "      <td>neal's back baby james neal hit on zach parise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0  Well, let's be honest here, they don't actuall...   \n",
       "1  Well, I didn't need evidence to believe in com...   \n",
       "2              Who does an \"official promo\" in 360p?   \n",
       "3                           Grotto koth was the best   \n",
       "4                                   Neal's back baby   \n",
       "\n",
       "                                      parent_comment  label  \\\n",
       "0  They should shut the fuck up and let the commu...      0   \n",
       "1  You need evidence to kill people? I thought we...      1   \n",
       "2                    2014 BMW S1000R: Official Promo      0   \n",
       "3  Not really that memorable lol if you want memo...      1   \n",
       "4                      James Neal hit on Zach Parise      1   \n",
       "\n",
       "                                         text_corpus  \n",
       "0  well, let's be honest here, they don't actuall...  \n",
       "1  well, i didn't need evidence to believe in com...  \n",
       "2  who does an \"official promo\" in 360p? 2014 bmw...  \n",
       "3  grotto koth was the best not really that memor...  \n",
       "4     neal's back baby james neal hit on zach parise  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sarc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bd5ab50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sarc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51d79a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    well, let's be honest here, they don't actuall...\n",
       "1    well, i didn't need evidence to believe in com...\n",
       "2    who does an \"official promo\" in 360p? 2014 bmw...\n",
       "3    grotto koth was the best not really that memor...\n",
       "4       neal's back baby james neal hit on zach parise\n",
       "Name: text_corpus, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the preprocess\n",
    "data_sarc.text_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bb660e",
   "metadata": {},
   "source": [
    "So all capital letters has been conerted into lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c2b8b7",
   "metadata": {},
   "source": [
    "### remove URLs from text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ad801b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sarc['text_corpus']=data_sarc['text_corpus'].replace('https:?//\\S+www\\.\\S+',' ',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f135f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        well, let's be honest here, they don't actuall...\n",
       "1        well, i didn't need evidence to believe in com...\n",
       "2        who does an \"official promo\" in 360p? 2014 bmw...\n",
       "3        grotto koth was the best not really that memor...\n",
       "4           neal's back baby james neal hit on zach parise\n",
       "                               ...                        \n",
       "14995    well with a name like el cubano i'm surprised ...\n",
       "14996    ... this is a good point. sounds like a pretty...\n",
       "14997    yep. i know the type you speak of. the \"die ci...\n",
       "14998    that's what the government wants you to believ...\n",
       "14999    because windows 10 has the glorious start menu...\n",
       "Name: text_corpus, Length: 15000, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sarc.text_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba07b21",
   "metadata": {},
   "source": [
    "### remove panctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56a7200d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02a9ebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove panctuations from text corpus\n",
    "data_sarc['text_corpus']=data_sarc['text_corpus'].str.translate(str.maketrans('', '',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c3aaf1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    well lets be honest here they dont actually se...\n",
       "1    well i didnt need evidence to believe in commu...\n",
       "2    who does an official promo in 360p 2014 bmw s1...\n",
       "3    grotto koth was the best not really that memor...\n",
       "4        neals back baby james neal hit on zach parise\n",
       "Name: text_corpus, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sarc.text_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59f16b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### using user defined function\n",
    "def remove_special_characters(text):\n",
    "    # Create a translation table with special characters mapped to None\n",
    "    translation_table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    # Remove special characters using the translation table\n",
    "    cleaned_text = text.translate(translation_table)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10043967",
   "metadata": {},
   "source": [
    "### remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71c43430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1208ac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sarc['text_corpus']=data_sarc['text_corpus'].str.replace('[^a-zA-Z0-9\\s]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1fd5a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        well lets be honest here they dont actually se...\n",
       "1        well i didnt need evidence to believe in commu...\n",
       "2        who does an official promo in 360p 2014 bmw s1...\n",
       "3        grotto koth was the best not really that memor...\n",
       "4            neals back baby james neal hit on zach parise\n",
       "                               ...                        \n",
       "14995    well with a name like el cubano im surprised h...\n",
       "14996     this is a good point sounds like a pretty goo...\n",
       "14997    yep i know the type you speak of the die cis s...\n",
       "14998    thats what the government wants you to believe...\n",
       "14999    because windows 10 has the glorious start menu...\n",
       "Name: text_corpus, Length: 15000, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sarc.text_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203926d5",
   "metadata": {},
   "source": [
    "### remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5483f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b448efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using gensim library\n",
    "data_sarc['text_corpus']=data_sarc['text_corpus'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35c4f49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    lets honest dont actually moderating spend tim...\n",
       "1    didnt need evidence believe communism need evi...\n",
       "2    official promo 360p 2014 bmw s1000r official p...\n",
       "3    grotto koth best memorable lol want memorable ...\n",
       "4                neals baby james neal hit zach parise\n",
       "Name: text_corpus, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sarc.text_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bb67f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using nltk\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baec15a",
   "metadata": {},
   "source": [
    "#### some other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ab830a",
   "metadata": {},
   "source": [
    "# Clean-up method for text features\n",
    "def clean_text(text):\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
    "\n",
    "    # Handle contractions\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"'s\", \" is\", text)\n",
    "    text = re.sub(r\"'re\", \" are\", text)\n",
    "    text = re.sub(r\"'d\", \" would\", text)\n",
    "    text = re.sub(r\"'ll\", \" will\", text)\n",
    "    text = re.sub(r\"'t\", \" not\", text)\n",
    "    text = re.sub(r\"'ve\", \" have\", text)\n",
    "    text = re.sub(r\"'m\", \" am\", text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = \" \".join(tokens)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6002b72",
   "metadata": {},
   "source": [
    "Create text corpus from student comment and parent comment\n",
    "\n",
    "data_sarc[\"comment\"] = data_sarc[\"comment\"].apply(clean_text)\n",
    "\n",
    "data_sarc[\"parent_comment\"] = data_sarc[\"parent_comment\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ae4ce2",
   "metadata": {},
   "source": [
    "### Explore Classics ML models for your NLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb6e8b6",
   "metadata": {},
   "source": [
    "Perform text to numeric conversion using CountVectorization and also with TF-IDF on the cleaned dataset. Now process the whole DataFrame (vectorize text + other features) with classic ML models for example- Logistic Regression, Naive Bayes, LDA, Decision tree etc. Tune you models and suggest what combination of vectorization technique & ML model is most suitable for the given data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d857f498",
   "metadata": {},
   "source": [
    "### creata a data frame of text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f61c04cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment', 'parent_comment', 'label', 'text_corpus'], dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the columns of data_sarc\n",
    "data_sarc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ed68834",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sarc=data_sarc.drop(['comment', 'parent_comment'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "378c12a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text_corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>lets honest dont actually moderating spend tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>didnt need evidence believe communism need evi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>official promo 360p 2014 bmw s1000r official p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>grotto koth best memorable lol want memorable ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>neals baby james neal hit zach parise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                        text_corpus\n",
       "0      0  lets honest dont actually moderating spend tim...\n",
       "1      1  didnt need evidence believe communism need evi...\n",
       "2      0  official promo 360p 2014 bmw s1000r official p...\n",
       "3      1  grotto koth best memorable lol want memorable ...\n",
       "4      1              neals baby james neal hit zach parise"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sarc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2159e4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sarc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19245c5e",
   "metadata": {},
   "source": [
    "### Build ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e9e8c8",
   "metadata": {},
   "source": [
    "#### Vectorization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c8fa6d",
   "metadata": {},
   "source": [
    "#### CountVectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55c506bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count vectorizer\n",
    "c_vecto=CountVectorizer()\n",
    "df_cvecto=c_vecto.fit_transform(df_sarc['text_corpus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6340757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numerical\n",
    "df_cvecto=df_cvecto.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff526341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print numerical values of dataframe using countvectorizer\n",
    "df_cvecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25b1a4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 34627)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the shape\n",
    "df_cvecto.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a1079",
   "metadata": {},
   "source": [
    "#### TF-Idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55806717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate Tfidf vectorizer\n",
    "tfidf_vecto=TfidfVectorizer()\n",
    "df_tfvecto=tfidf_vecto.fit_transform(df_sarc['text_corpus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b0e6db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it to numerical\n",
    "df_tfvecto=df_tfvecto.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3aa68581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print tfidf vectorizer numerical value\n",
    "df_tfvecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2947265f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 34627)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the shape\n",
    "df_tfvecto.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47222fd9",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d30d10",
   "metadata": {},
   "source": [
    "### Logistic Regression model using countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bbd103a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split both countvectorized & TF-idf dataset into train and test set with standard split ratio method\n",
    "#target column for both vectorizer\n",
    "Y=df_sarc['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1fa558a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "22c08083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    0\n",
       "3    1\n",
       "4    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb1ac75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cvecto,x_test_cvecto,y_train_cvecto,y_test_cvecto=train_test_split(df_cvecto,Y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b136080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "lr_cvecto=LogisticRegression()\n",
    "lr_cvecto.fit(x_train_cvecto,y_train_cvecto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47b3c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find accurcy score for logistic countvectorizer\n",
    "y_true=y_test_cvecto\n",
    "y_pred_cvecto=lr_cvecto.predict(x_test_cvecto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cfed21c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5936666666666667"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the accuracy score\n",
    "accuracy_score(y_true,y_pred_cvecto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7bce15",
   "metadata": {},
   "source": [
    "### Logistic Regression model using TFidfvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e044ec7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e6cde0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfvecto,x_test_tfvecto,y_train_tfvecto,y_test_tfvecto=train_test_split(df_tfvecto,Y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "daf69295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "lr_tfvecto=LogisticRegression()\n",
    "lr_tfvecto.fit(x_train_tfvecto,y_train_tfvecto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "939e2eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find accurcy score for logistic countvectorizer\n",
    "y_true_tfvecto=y_test_tfvecto\n",
    "y_pred_tfvecto=lr_tfvecto.predict(x_test_tfvecto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a17efdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6056666666666667"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the accuracy score\n",
    "accuracy_score(y_true_tfvecto,y_pred_tfvecto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926de04e",
   "metadata": {},
   "source": [
    "## Navie bayes Model & count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5f86b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "552a5bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using navie bayes model\n",
    "nv_cvecto= MultinomialNB()\n",
    "nv_cvecto.fit(x_train_cvecto,y_train_cvecto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9f736345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find accurcy score for navie bayes & countvectorizer\n",
    "y_true_nv_cvecto=y_test_cvecto\n",
    "y_pred_nv_cvecto=nv_cvecto.predict(x_test_cvecto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2e54915c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5866666666666667"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true_nv_cvecto,y_pred_nv_cvecto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4043f0d",
   "metadata": {},
   "source": [
    "## Navie bayes Model & TFidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b8e3e1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using navie bayes model\n",
    "nv_tfvecto= MultinomialNB()\n",
    "nv_tfvecto.fit(x_train_tfvecto,y_train_tfvecto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d5017ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find accurcy score for navie bayes & countvectorizer\n",
    "y_true_nv_tfvecto=y_test_tfvecto\n",
    "y_pred_nv_tfvecto=nv_tfvecto.predict(x_test_tfvecto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca21fcfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.586"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true_nv_tfvecto,y_pred_nv_tfvecto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8cccbe",
   "metadata": {},
   "source": [
    "### LDA model using count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d49c7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c72baf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 70:30 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_sarc['text_corpus'], data_sarc['label'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9b5e03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "c_vectorizer = CountVectorizer()\n",
    "X_train_lda_cvecto = c_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1900b640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation()"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply LDA\n",
    "lda = LatentDirichletAllocation()\n",
    "lda.fit(X_train_lda_cvecto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "66d7d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test data into the same format\n",
    "X_test_lda_cvecto = c_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b4d57060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels for the test data\n",
    "y_pred_lda_cvecto = lda.transform(X_test_lda_cvecto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3270e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the predicted topic probabilities into predicted labels\n",
    "y_pred = y_pred_lda_cvecto.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "eda2b0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy of the model\n",
    "accuracy = (y_pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c1b85cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14444444444444443"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c87a9b7",
   "metadata": {},
   "source": [
    "### LDA model using Tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "624ec236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "tf_vectorizer = TfidfVectorizer()\n",
    "X_train_lda_tfvecto = tf_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f3948fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation()"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply LDA\n",
    "lda = LatentDirichletAllocation()\n",
    "lda.fit(X_train_lda_tfvecto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "09fffe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test data into the same format\n",
    "X_test_lda_tfvecto = tf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5ad6eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels for the test data\n",
    "y_pred_lda_tfvecto = lda.transform(X_test_lda_tfvecto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e0f122fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the predicted topic probabilities into predicted labels\n",
    "y_pred = y_pred_lda_tfvecto.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ab016ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy of the model\n",
    "accuracy = (y_pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e56695a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06288888888888888"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47db1566",
   "metadata": {},
   "source": [
    "### Explore Word Embeddings & classic DL models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d5a9b1",
   "metadata": {},
   "source": [
    "Preform text vectorization using word embedding techniques (Word2Vec & Glove). Now use the embedding to build DL models such as RNN, LSTM & Bi-LSTM. Tune you models and suggest what combination of embedding technique and DL model is most suitable for the given data set. Now for the finalized embedding method use its pretrained word embeddings and test if model performance can be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1330c721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\vinay\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from gensim) (1.9.3)\n",
      "Requirement already satisfied: keras in c:\\users\\vinay\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: keras in c:\\users\\vinay\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Collecting keras\n",
      "  Using cached keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.11.0\n",
      "    Uninstalling keras-2.11.0:\n",
      "      Successfully uninstalled keras-2.11.0\n",
      "Successfully installed keras-2.12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.11.0 requires keras<2.12,>=2.11.0, but you have keras 2.12.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install keras\n",
    "!pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "28651387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: tensorflow in c:\\users\\vinay\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: keras in c:\\users\\vinay\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.51.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (23.3.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.5)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall keras tensorflow\n",
    "!pip install tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dfef1071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "90dbcf47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text_corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>lets honest dont actually moderating spend tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>didnt need evidence believe communism need evi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>official promo 360p 2014 bmw s1000r official p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>grotto koth best memorable lol want memorable ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>neals baby james neal hit zach parise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                        text_corpus\n",
       "0      0  lets honest dont actually moderating spend tim...\n",
       "1      1  didnt need evidence believe communism need evi...\n",
       "2      0  official promo 360p 2014 bmw s1000r official p...\n",
       "3      1  grotto koth best memorable lol want memorable ...\n",
       "4      1              neals baby james neal hit zach parise"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sarc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ba892d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any unnecessary columns\n",
    "dataset = df_sarc[['text_corpus', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9cc927fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_corpus</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lets honest dont actually moderating spend tim...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>didnt need evidence believe communism need evi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>official promo 360p 2014 bmw s1000r official p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grotto koth best memorable lol want memorable ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neals baby james neal hit zach parise</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text_corpus  label\n",
       "0  lets honest dont actually moderating spend tim...      0\n",
       "1  didnt need evidence believe communism need evi...      1\n",
       "2  official promo 360p 2014 bmw s1000r official p...      0\n",
       "3  grotto koth best memorable lol want memorable ...      1\n",
       "4              neals baby james neal hit zach parise      1"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6c5ef8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 2)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "756a490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(dataset['text_corpus'], dataset['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "744a9dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text vectorization using Word2Vec\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ae226c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to sequences of word indexes\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "72be22bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding sequences to the same length\n",
    "max_sequence_length = max([len(sequence) for sequence in train_sequences])\n",
    "train_data = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
    "test_data = pad_sequences(test_sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e0a66da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Word2Vec model\n",
    "w2v_model = Word2Vec(train_texts, vector_size=100, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb140073",
   "metadata": {},
   "source": [
    "### Embedding and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b65b8e89",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Create word embeddings\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1300a6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 223s 589ms/step - loss: 0.6934 - accuracy: 0.5056 - val_loss: 0.6935 - val_accuracy: 0.5077\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 208s 556ms/step - loss: 0.6932 - accuracy: 0.5002 - val_loss: 0.6928 - val_accuracy: 0.5060\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 170s 454ms/step - loss: 0.6929 - accuracy: 0.4974 - val_loss: 0.6929 - val_accuracy: 0.5060\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 186s 496ms/step - loss: 0.6925 - accuracy: 0.4989 - val_loss: 0.6929 - val_accuracy: 0.5077\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 172s 459ms/step - loss: 0.6926 - accuracy: 0.5067 - val_loss: 0.6929 - val_accuracy: 0.4987\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 163s 435ms/step - loss: 0.6921 - accuracy: 0.5055 - val_loss: 0.6929 - val_accuracy: 0.5090\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 161s 430ms/step - loss: 0.6917 - accuracy: 0.5076 - val_loss: 0.6932 - val_accuracy: 0.4950\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 159s 424ms/step - loss: 0.6914 - accuracy: 0.5071 - val_loss: 0.6936 - val_accuracy: 0.4953\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 172s 458ms/step - loss: 0.6911 - accuracy: 0.5057 - val_loss: 0.6939 - val_accuracy: 0.5077\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 218s 581ms/step - loss: 0.6913 - accuracy: 0.5035 - val_loss: 0.6930 - val_accuracy: 0.5140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29615efd9a0>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model building and training\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, train_labels, validation_data=(test_data, test_labels), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9c74fba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 22s 226ms/step\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "predictions = model.predict(test_data)\n",
    "predictions = (predictions > 0.5).astype(int)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "precision = precision_score(test_labels, predictions)\n",
    "recall = recall_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4a4c499b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.514\n",
      "Precision: 0.5085514834205934\n",
      "Recall: 0.966821499668215\n",
      "F1 Score: 0.6665141811527906\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2751a63",
   "metadata": {},
   "source": [
    "### Embedding and RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "21e6cb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 82s 216ms/step - loss: 0.7011 - accuracy: 0.5013 - val_loss: 0.6942 - val_accuracy: 0.4950\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 57s 152ms/step - loss: 0.6989 - accuracy: 0.4978 - val_loss: 0.6960 - val_accuracy: 0.4973\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 49s 131ms/step - loss: 0.6978 - accuracy: 0.5098 - val_loss: 0.6969 - val_accuracy: 0.5027\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 52s 138ms/step - loss: 0.6964 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5050\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 60s 161ms/step - loss: 0.6958 - accuracy: 0.4966 - val_loss: 0.6959 - val_accuracy: 0.4980\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 57s 153ms/step - loss: 0.6950 - accuracy: 0.5044 - val_loss: 0.6931 - val_accuracy: 0.4970\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 57s 151ms/step - loss: 0.6958 - accuracy: 0.4973 - val_loss: 0.6929 - val_accuracy: 0.4977\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 61s 162ms/step - loss: 0.6948 - accuracy: 0.5107 - val_loss: 0.6975 - val_accuracy: 0.4977\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 57s 152ms/step - loss: 0.6950 - accuracy: 0.5001 - val_loss: 0.6950 - val_accuracy: 0.4993\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 56s 149ms/step - loss: 0.6951 - accuracy: 0.5013 - val_loss: 0.6960 - val_accuracy: 0.4987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2961bb416a0>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "# Model building and training\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n",
    "model.add(SimpleRNN(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, train_labels, validation_data=(test_data, test_labels), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1d030651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 4s 38ms/step\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "predictions = model.predict(test_data)\n",
    "predictions = (predictions > 0.5).astype(int)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "precision = precision_score(test_labels, predictions)\n",
    "recall = recall_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "47f8b189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.49866666666666665\n",
      "Precision: 0.5714285714285714\n",
      "Recall: 0.007962840079628402\n",
      "F1 Score: 0.015706806282722516\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dcd878",
   "metadata": {},
   "source": [
    "### Embedding and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bcc5f726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 195s 514ms/step - loss: 0.6936 - accuracy: 0.5073 - val_loss: 0.6932 - val_accuracy: 0.4983\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 191s 509ms/step - loss: 0.6930 - accuracy: 0.5061 - val_loss: 0.6930 - val_accuracy: 0.4977\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 185s 493ms/step - loss: 0.6927 - accuracy: 0.5017 - val_loss: 0.6928 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 193s 515ms/step - loss: 0.6946 - accuracy: 0.5023 - val_loss: 0.6938 - val_accuracy: 0.4997\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 161s 428ms/step - loss: 0.6922 - accuracy: 0.5118 - val_loss: 0.6934 - val_accuracy: 0.4963\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 162s 432ms/step - loss: 0.6920 - accuracy: 0.5064 - val_loss: 0.6930 - val_accuracy: 0.5107\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 158s 420ms/step - loss: 0.6916 - accuracy: 0.5057 - val_loss: 0.6939 - val_accuracy: 0.5120\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 156s 417ms/step - loss: 0.6916 - accuracy: 0.5023 - val_loss: 0.6931 - val_accuracy: 0.5097\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 155s 413ms/step - loss: 0.6916 - accuracy: 0.5009 - val_loss: 0.6932 - val_accuracy: 0.5117\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 1358s 4s/step - loss: 0.6911 - accuracy: 0.5011 - val_loss: 0.6932 - val_accuracy: 0.5080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2961bea99d0>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model building and training\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, train_labels, validation_data=(test_data, test_labels), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "09579c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 20s 210ms/step\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "predictions = model.predict(test_data)\n",
    "predictions = (predictions > 0.5).astype(int)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "precision = precision_score(test_labels, predictions)\n",
    "recall = recall_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fd9c9f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.508\n",
      "Precision: 0.5055101315321721\n",
      "Recall: 0.9435965494359655\n",
      "F1 Score: 0.6583333333333334\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e757ec7",
   "metadata": {},
   "source": [
    "### Embedding and Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b7804aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 405s 1s/step - loss: 0.6935 - accuracy: 0.5081 - val_loss: 0.6931 - val_accuracy: 0.5047\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 390s 1s/step - loss: 0.6931 - accuracy: 0.5002 - val_loss: 0.6929 - val_accuracy: 0.4973\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 436s 1s/step - loss: 0.6929 - accuracy: 0.4991 - val_loss: 0.6929 - val_accuracy: 0.5100\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 406s 1s/step - loss: 0.6926 - accuracy: 0.5033 - val_loss: 0.6932 - val_accuracy: 0.5097\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 397s 1s/step - loss: 0.6962 - accuracy: 0.5033 - val_loss: 0.6933 - val_accuracy: 0.5047\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 1778s 5s/step - loss: 0.6939 - accuracy: 0.5040 - val_loss: 0.6932 - val_accuracy: 0.5067\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 326s 870ms/step - loss: 0.6938 - accuracy: 0.5076 - val_loss: 0.6932 - val_accuracy: 0.5023\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 353s 941ms/step - loss: 0.6947 - accuracy: 0.5030 - val_loss: 0.6940 - val_accuracy: 0.4987\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 331s 883ms/step - loss: 0.6941 - accuracy: 0.5005 - val_loss: 0.6933 - val_accuracy: 0.5060\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 331s 882ms/step - loss: 0.6939 - accuracy: 0.5061 - val_loss: 0.6932 - val_accuracy: 0.4980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2961d9d0b80>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model building and training\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, train_labels, validation_data=(test_data, test_labels), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b35614f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 21s 222ms/step\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "predictions = model.predict(test_data)\n",
    "predictions = (predictions > 0.5).astype(int)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "precision = precision_score(test_labels, predictions)\n",
    "recall = recall_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "edb01d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.498\n",
      "Precision: 0.5066666666666667\n",
      "Recall: 0.025215660252156602\n",
      "F1 Score: 0.04804045512010113\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b901bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5fe051ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "880fef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d77c081",
   "metadata": {},
   "source": [
    "## Explore State-of the Art Transformer models\n",
    "Use any two state-of-the-art transformer models and check if you can improve NLP model performance further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8413670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "017b0be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4bc5f37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "     ---------------------------------------- 7.1/7.1 MB 28.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "     ---------------------------------------- 3.5/3.5 MB 55.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "     ---------------------------------------- 224.5/224.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.3.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2022.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\vinay\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "076486c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, GPT2Tokenizer, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b01cd772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(dataset['text_corpus'], dataset['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "0afc6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and tokenize the BERT model\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_sequences = bert_tokenizer.batch_encode_plus(\n",
    "    train_texts.tolist(),\n",
    "    padding='longest',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b6650045",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = bert_tokenizer.batch_encode_plus(\n",
    "    test_texts.tolist(),\n",
    "    padding='longest',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c27c49b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and tokenize the GPT model\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "train_sequences_gpt = gpt_tokenizer.batch_encode_plus(\n",
    "    train_texts.tolist(),\n",
    "    padding='longest',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9529227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences_gpt = gpt_tokenizer.batch_encode_plus(\n",
    "    test_texts.tolist(),\n",
    "    padding='longest',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "bd746b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized sequences to numpy arrays\n",
    "train_data = np.array(train_sequences['input_ids'])\n",
    "test_data = np.array(test_sequences['input_ids'])\n",
    "train_data_gpt = np.array(train_sequences_gpt['input_ids'])\n",
    "test_data_gpt = np.array(test_sequences_gpt['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "26c3980e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f485cf34410e400fabccee10e2092231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Define the BERT model\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9507c467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get BERT embeddings\n",
    "train_embeddings = []\n",
    "for batch in train_data:\n",
    "    input_ids = torch.tensor([batch])\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = bert_model(input_ids)[0]\n",
    "    train_embeddings.append(last_hidden_states.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f56fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = []\n",
    "for batch in test_data:\n",
    "    input_ids = torch.tensor([batch])\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = bert_model(input_ids)[0]\n",
    "    test_embeddings.append(last_hidden_states.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7903f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GPT model\n",
    "gpt_model = GPT2Model.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166904c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GPT embeddings\n",
    "train_embeddings_gpt = []\n",
    "for batch in train_data_gpt:\n",
    "    input_ids = torch.tensor([batch])\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = gpt_model(input_ids)[0]\n",
    "    train_embeddings_gpt.append(last_hidden_states.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1970310",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings_gpt = []\n",
    "for batch in test_data_gpt:\n",
    "    input_ids = torch.tensor([batch])\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = gpt_model(input_ids)[0]\n",
    "    test_embeddings_gpt.append(last_hidden_states.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cee65e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the embeddings\n",
    "train_embeddings = np.concatenate(train_embeddings, axis=0)\n",
    "test_embeddings = np.concatenate(test_embeddings, axis=0)\n",
    "train_embeddings_gpt = np.concatenate(train_embeddings_gpt, axis=0)\n",
    "test_embeddings_gpt = np.concatenate(test_embeddings_gpt, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate BERT and GPT embeddings\n",
    "train_embeddings_combined = np.concatenate([train_embeddings, train_embeddings_gpt], axis=1)\n",
    "test_embeddings_combined = np.concatenate([test_embeddings, test_embeddings_gpt], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building and training\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_dim=train_embeddings_combined.shape[1]))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a38bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "model.fit(train_embeddings_combined, train_labels, validation_data=(test_embeddings_combined, test_labels), epochs=10, batch_size=32, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eb4f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "predictions = model.predict(test_embeddings_combined)\n",
    "predictions = (predictions > 0.5).astype(int)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "precision = precision_score(test_labels, predictions)\n",
    "recall = recall_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e327b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae915fae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
